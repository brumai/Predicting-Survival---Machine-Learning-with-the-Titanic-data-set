{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Setup Import Libs\n",
    "We will import sklearn models as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d3cb1c42-90ba-9674-0e72-0b4ee496fa42"
   },
   "outputs": [],
   "source": [
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Handle table-like data and matrices\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Modelling Helpers\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.preprocessing import Normalizer , scale\n",
    "from sklearn.cross_validation import train_test_split , StratifiedKFold\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "# Visualisation\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "import seaborn as sns\n",
    "\n",
    "# Configure visualisations\n",
    "%matplotlib inline\n",
    "mpl.style.use( 'ggplot' )\n",
    "sns.set_style( 'white' )\n",
    "pylab.rcParams[ 'figure.figsize' ] = 8 , 6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "cb44cb03-a5be-653d-34bf-f1e7eba133a4"
   },
   "source": [
    "## 1.2 Setup helper Functions\n",
    "There is no need to understand this code. Just run it to simplify the code later in the tutorial.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "0bb8cf49-d080-46a7-2c66-fa967ad4db97"
   },
   "outputs": [],
   "source": [
    "def plot_histograms( df , variables , n_rows , n_cols ):\n",
    "    fig = plt.figure( figsize = ( 16 , 12 ) )\n",
    "    for i, var_name in enumerate( variables ):\n",
    "        ax=fig.add_subplot( n_rows , n_cols , i+1 )\n",
    "        df[ var_name ].hist( bins=10 , ax=ax )\n",
    "        ax.set_title( 'Skew: ' + str( round( float( df[ var_name ].skew() ) , ) ) ) # + ' ' + var_name ) #var_name+\" Distribution\")\n",
    "        ax.set_xticklabels( [] , visible=False )\n",
    "        ax.set_yticklabels( [] , visible=False )\n",
    "    fig.tight_layout()  # Improves appearance a bit.\n",
    "    plt.show()\n",
    "\n",
    "def plot_distribution( df , var , target , **kwargs ):\n",
    "    row = kwargs.get( 'row' , None )\n",
    "    col = kwargs.get( 'col' , None )\n",
    "    facet = sns.FacetGrid( df , hue=target , aspect=4 , row = row , col = col )\n",
    "    facet.map( sns.kdeplot , var , shade= True )\n",
    "    facet.set( xlim=( 0 , df[ var ].max() ) )\n",
    "    facet.add_legend()\n",
    "\n",
    "def plot_categories( df , cat , target , **kwargs ):\n",
    "    row = kwargs.get( 'row' , None )\n",
    "    col = kwargs.get( 'col' , None )\n",
    "    facet = sns.FacetGrid( df , row = row , col = col )\n",
    "    facet.map( sns.barplot , cat , target )\n",
    "    facet.add_legend()\n",
    "\n",
    "def plot_correlation_map( df ):\n",
    "    corr = titanic.corr()\n",
    "    _ , ax = plt.subplots( figsize =( 12 , 10 ) )\n",
    "    cmap = sns.diverging_palette( 220 , 10 , as_cmap = True )\n",
    "    _ = sns.heatmap(\n",
    "        corr, \n",
    "        cmap = cmap,\n",
    "        square=True, \n",
    "        cbar_kws={ 'shrink' : .9 }, \n",
    "        ax=ax, \n",
    "        annot = True, \n",
    "        annot_kws = { 'fontsize' : 12 }\n",
    "    )\n",
    "\n",
    "def describe_more( df ):\n",
    "    var = [] ; l = [] ; t = []\n",
    "    for x in df:\n",
    "        var.append( x )\n",
    "        l.append( len( pd.value_counts( df[ x ] ) ) )\n",
    "        t.append( df[ x ].dtypes )\n",
    "    levels = pd.DataFrame( { 'Variable' : var , 'Levels' : l , 'Datatype' : t } )\n",
    "    levels.sort_values( by = 'Levels' , inplace = True )\n",
    "    return levels\n",
    "\n",
    "def plot_variable_importance( X , y ):\n",
    "    tree = DecisionTreeClassifier( random_state = 99 )\n",
    "    tree.fit( X , y )\n",
    "    plot_model_var_imp( tree , X , y )\n",
    "    \n",
    "def plot_model_var_imp( model , X , y ):\n",
    "    imp = pd.DataFrame( \n",
    "        model.feature_importances_  , \n",
    "        columns = [ 'Importance' ] , \n",
    "        index = X.columns \n",
    "    )\n",
    "    imp = imp.sort_values( [ 'Importance' ] , ascending = True )\n",
    "    imp[ : 10 ].plot( kind = 'barh' )\n",
    "    print (model.score( X , y ))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ea8b0e99-e512-f1f5-ed3d-e7df876b9bed"
   },
   "source": [
    "## 1.3 Import data\n",
    "Now that our packages are loaded, let's read in and take a peek at the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ee2677e2-b78b-250b-a908-816109e3ab91"
   },
   "outputs": [],
   "source": [
    "# get titanic & test csv files as a DataFrame\n",
    "titanic = pd.read_csv(\"data/train.csv\")\n",
    "test    = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "full = titanic.append( test , ignore_index = True )\n",
    "\n",
    "print ('Datasets:' , 'full:' , full.shape , 'titanic:' , titanic.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "76852c59-23bf-55b6-5b6d-f672c97114ae"
   },
   "source": [
    "## 2.1 Getting to know the data\n",
    "\n",
    "To understand the data we are now going to consider some key facts about various variables including their relationship with the target variable, i.e. survival.\n",
    "\n",
    "We start by looking at a few lines of the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "60d5efad-7649-c1e8-3cae-59bf562e8457"
   },
   "outputs": [],
   "source": [
    "# Run the code to see the variables, then read the variable description below to understand them.\n",
    "titanic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1f3708e1-b5ca-7540-39fc-da7453d0fb80"
   },
   "source": [
    "**VARIABLE DESCRIPTIONS:**\n",
    "\n",
    "We've got a sense of our variables, their class type, and the first few observations of each. We know we're working with 1309 observations of 12 variables. To make things a bit more explicit since a couple of the variable names aren't 100% illuminating, here's what we've got to deal with:\n",
    "\n",
    "\n",
    "**Variable Description**\n",
    "\n",
    " - Survived: Survived (1) or died (0)\n",
    " - Pclass: Passenger's class\n",
    " - Name: Passenger's name\n",
    " - Sex: Passenger's sex\n",
    " - Age: Passenger's age\n",
    " - SibSp: Number of siblings/spouses aboard\n",
    " - Parch: Number of parents/children aboard\n",
    " - Ticket: Ticket number\n",
    " - Fare: Fare\n",
    " - Cabin: Cabin\n",
    " - Embarked: Port of embarkation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "bcc371f8-8fbb-a582-5944-8537c152c6c9"
   },
   "source": [
    "### 2.2 Next have a look at some key information about the variables\n",
    "An numeric variable is one with values of integers or real numbers while a categorical variable is a variable that can take on one of a limited, and usually fixed, number of possible values, such as blood type.\n",
    "\n",
    "Notice especially what type of variable each is, how many observations there are and some of the variable values.\n",
    "\n",
    "An interesting observation could for example be the minimum age 0.42, do you know why this is?\n",
    "\n",
    "*Select the cell below and run it by pressing the play button.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "88c8b958-0973-d27e-d463-58fada41900e"
   },
   "outputs": [],
   "source": [
    "titanic.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.describe(include=['O'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c006f42b-cba3-7109-92fb-79676f726afb"
   },
   "source": [
    "### 2.3 Data Exploration\n",
    "PLot correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "84418f3e-32ab-7a6c-a60e-6d45760ee666"
   },
   "outputs": [],
   "source": [
    "plot_correlation_map(titanic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlations are not the whole story as it only picks up linear dependencies "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "952846c2-63a9-1923-2501-d3036056855b"
   },
   "source": [
    "### 2.3.1 Let's further explore the relationship between the features and survival of passengers \n",
    "We start by looking at the relationship between age and survival.\n",
    "\n",
    "*Select the cell below and run it by pressing the play button.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "617703c7-4ab2-186c-f4b1-40d5ffa37850"
   },
   "outputs": [],
   "source": [
    "# Plot distributions of Age of passangers who survived or did not survive\n",
    "plot_distribution( titanic , var = 'Age' , target = 'Survived', row='Sex')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "834c62d5-23e7-7e0c-d2d4-bc960d09a409"
   },
   "source": [
    "Consider the graphs above. Differences between survival for different values is what will be used to separate the target variable (survival in this case) in the model. If the two lines had been about the same, then it would not have been a good variable for our predictive model. \n",
    "\n",
    "Consider some key questions such as; what age does males/females have a higher or lower probability of survival? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f8d077bf-e70c-29df-253e-5bd22ff3f06e"
   },
   "source": [
    "### 2.3.2 Excersise 1: Investigating the other numeric variables\n",
    "It's time to get your hands dirty and do some coding! Try to plot the distributions of Fare of passangers who survived or did not survive. Then consider if this could be a good predictive variable.\n",
    "\n",
    "*Hint: use the code from the previous cell as a starting point.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6fcddc9f-19be-c474-a639-79c6b2d2a41a"
   },
   "outputs": [],
   "source": [
    "# In the this cell plot distributions of Fare of passangers who survived or did not survive\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e9dffe91-ee29-9cca-2860-29a32a44e2af"
   },
   "source": [
    "### 2.4.4 Embarked\n",
    "We can also look at categorical variables like sex and their relationship with survival.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic[['Sex', 'Survived']].groupby(['Sex'], as_index=True).mean().sort_values(by='Survived', ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate other categorical variables\n",
    "Pclass, SibSp, Embarked and Parch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embarked\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f5b4252e-4800-d674-f86f-01eba244230d"
   },
   "outputs": [],
   "source": [
    "# Parch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SibSp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c9888e98-091a-d98c-6ccf-b733a4f0d941"
   },
   "source": [
    "# 3. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e280f761-4b30-9776-09e3-fcaecf932a01"
   },
   "source": [
    "## 3.1 Categorical variables need to be transformed to numeric variables\n",
    "The variables *Embarked*, *Pclass* and *Sex* are treated as categorical variables. Some of our model algorithms can only handle numeric values and so we need to create a new variable (dummy variable) for every unique value of the categorical variables.\n",
    "\n",
    "This variable will have a value 1 if the row has a particular value and a value 0 if not. *Sex* is a dichotomy (old school gender theory) and will be encoded as one binary variable (0 or 1).\n",
    "\n",
    "*Select the cells below and run it by pressing the play button.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "75023b46-ded0-94ab-3ea1-7da1eec945fe"
   },
   "outputs": [],
   "source": [
    "# Transform Sex into binary values 0 and 1\n",
    "def prepSex(df):\n",
    "    return pd.Series( np.where( full.Sex == 'male' , 1 , 0 ), name = 'Sex' )\n",
    "sex = prepSex(full)\n",
    "sex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "cf816542-c40f-abe0-b662-a91211882bff"
   },
   "outputs": [],
   "source": [
    "# Create a new variable for every unique value of Embarked (binary encoding)\n",
    "# hint look at the pandas.get_dummies method\n",
    "def prepEmbarked(df):\n",
    "    pass\n",
    "embarked = prepEmbarked(full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "01758920-20cc-e200-90aa-e4404c41a8d7"
   },
   "outputs": [],
   "source": [
    "# Create a new variable for every unique value of Pclass (binary encoding)\n",
    "# hint look at the pandas.get_dummies method\n",
    "def prepPclass(df):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d1269afc-b929-d519-9646-146f8a91b472"
   },
   "source": [
    "## 3.2 Fill missing values in variables (Data Imputation)\n",
    "Most machine learning alghorims require all variables to have values in order to use it for training the model. The simplest method is to fill missing values with the average of the variable across all observations in the training set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "04044a9c-603d-1963-34f4-a85efb8c9166"
   },
   "outputs": [],
   "source": [
    "\n",
    "# hint use the pandas fillna method \n",
    "def imputeAgeViaMean(df):\n",
    "    pass\n",
    "\n",
    "def imputeFareViaMean(df):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4f2ce8f7-8bf6-69f8-e2ae-5593b5cf98ac"
   },
   "source": [
    "## 3.3 Feature Engineering &ndash; Creating new variables\n",
    "Credit: http://ahmedbesbes.com/how-to-score-08134-in-titanic-kaggle-challenge.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "bd8efd0f-4af4-d709-cfeb-1907f885e341"
   },
   "source": [
    "### 3.3.1 Extract titles from passenger names\n",
    "Titles reflect social status and may predict survival probability\n",
    "\n",
    "*Select the cell below and run it by pressing the play button.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "32772d9e-8c05-4eaf-5cd3-8425ae5837cd"
   },
   "outputs": [],
   "source": [
    "Title_Dictionary = {\n",
    "                    \"Capt\":       \"Officer\",\n",
    "                    \"Col\":        \"Officer\",\n",
    "                    \"Major\":      \"Officer\",\n",
    "                    \"Jonkheer\":   \"Royalty\",\n",
    "                    \"Don\":        \"Royalty\",\n",
    "                    \"Sir\" :       \"Royalty\",\n",
    "                    \"Dr\":         \"Officer\",\n",
    "                    \"Rev\":        \"Officer\",\n",
    "                    \"the Countess\":\"Royalty\",\n",
    "                    \"Dona\":       \"Royalty\",\n",
    "                    \"Mme\":        \"Mrs\",\n",
    "                    \"Mlle\":       \"Miss\",\n",
    "                    \"Ms\":         \"Mrs\",\n",
    "                    \"Mr\" :        \"Mr\",\n",
    "                    \"Mrs\" :       \"Mrs\",\n",
    "                    \"Miss\" :      \"Miss\",\n",
    "                    \"Master\" :    \"Master\",\n",
    "                    \"Lady\" :      \"Royalty\"\n",
    "                    }\n",
    "\n",
    "# create a function that extracts the title from name and return pd.Series of titles\n",
    "def extractTitle(df):\n",
    "    pass\n",
    "\n",
    "title = extractTitle(full)\n",
    "title.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepTitle(df):\n",
    "    # hint https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.map.html with Title_Dictionary\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# a map of more aggregated titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a56838a1-5d47-194f-7ac6-06c397b60482"
   },
   "source": [
    "### 3.3.2 Extract Cabin category information from the Cabin number\n",
    "\n",
    "*Select the cell below and run it by pressing the play button.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "87ed124f-d659-bf03-7dbf-accee0ddfe75"
   },
   "outputs": [],
   "source": [
    "# come back to this later if you want to see if you can make further improvements to the model \n",
    "def prepCabin(df):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d2dba1d1-e7cd-b9c8-fcc7-fae22cca38da"
   },
   "source": [
    "### 3.3.3 Extract ticket class from ticket number\n",
    "\n",
    "*Select the cell below and run it by pressing the play button.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "5319db41-01dd-3a48-83d8-56572272d966"
   },
   "outputs": [],
   "source": [
    "# come back to this later if you want to see if you can make further improvements to the model \n",
    "def prepTicket(df):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7e812a29-3885-30a9-3d51-beda531016d9"
   },
   "source": [
    "### 3.3.4 Create family size and category for family size\n",
    "The two variables *Parch* and *SibSp* are used to create the famiy size variable\n",
    "\n",
    "*Select the cell below and run it by pressing the play button.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d885986a-77ea-b35c-5ddc-c16f54357232"
   },
   "outputs": [],
   "source": [
    "# make a function that extracts family size as a pd.Series, a family size of 1  should indicate the individual is on their own\n",
    "def familySize(df):\n",
    "    pass\n",
    "\n",
    "\n",
    "# make a function that extracts returns 3 pd.Series ta t are binary feautres signalling whether an individual \n",
    "def prepFamilyFeatures(df):\n",
    "    pass\n",
    "familyFeatures = prepFamilyFeatures(full)\n",
    "\n",
    "\n",
    "familyFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c20b06e8-7126-a80e-762a-8a383ffba9e1"
   },
   "source": [
    "## 3.4 Assemble final datasets for modelling\n",
    "\n",
    "Split dataset by rows into test and train in order to have a holdout set to do model evaluation on. The dataset is also split by columns in a matrix (X) containing the input data and a vector (y) containing the target (or labels)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "29663deb-5bbf-b621-abe5-37cfa5e3d29f"
   },
   "source": [
    "### 3.4.1 Variable selection\n",
    "Select which features/variables to inculde in the dataset from the list below:\n",
    "\n",
    " - age\n",
    " - embarked\n",
    " - pclass\n",
    " - sex\n",
    " - family\n",
    " - cabin\n",
    " - ticket\n",
    "\n",
    "*Include the variables you would like to use in the function below seperated by comma, then run the cell*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "4b3459bd-752c-0a96-22dc-8672900a0bb9"
   },
   "outputs": [],
   "source": [
    "# Select which features/variables to include in the dataset from the list below:\n",
    "# imputed fare and age, embarked features, pclass , sex , family featrues, cabin , ticket\n",
    "def prepData(df):\n",
    "    return pd.concat([prepTitle(df),\n",
    "                      prepFamilyFeatures(df),\n",
    "                      imputeFareViaMean(df),\n",
    "                      imputeAgeViaMean(df),\n",
    "                      prepPclass(df),\n",
    "                      prepEmbarked(df),\n",
    "                      prepSex(df)\n",
    "                     ], axis=1)\n",
    "full_use = shuffle(full, random_state=0)\n",
    "full_X = prepData(full_use)\n",
    "full_y = full.Survived\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "cb10186c-81b3-f9cd-250a-fab46b0001f5"
   },
   "source": [
    "### 3.4.2 Create datasets\n",
    "Below we will seperate the data into training and test datasets.\n",
    "\n",
    "*Select the cell below and run it by pressing the play button.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "7ca114b3-f22c-b8bf-4e61-9c9bcd82fa63"
   },
   "outputs": [],
   "source": [
    "# Create all datasets that are necessary to train, validate and test models\n",
    "train_X_full = full_X[ 0:891 ]\n",
    "train_y_full = full_y[ 0:891 ]\n",
    "test_X = full_X[ 891: ]\n",
    "test_y = full_y[ 891: ]\n",
    "\n",
    "print (train_X.shape , train_y.shape , test_X.shape , test_y.shape)\n",
    "\n",
    "train_X , valid_X , train_y , valid_y = train_test_split( train_X_full , train_y_full , train_size = .7 )\n",
    "\n",
    "\n",
    "\n",
    "print (full_X.shape , train_X.shape , valid_X.shape , train_y.shape , valid_y.shape , test_X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f269d9b9-73fc-6d34-b46a-61fd6f1357ae"
   },
   "source": [
    "### 3.4.3 Feature importance\n",
    "Selecting the optimal features in the model is important. \n",
    "We will now try to evaluate what the most important variables are for the model to make the prediction.\n",
    "\n",
    "*Select the cell below and run it by pressing the play button.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "17c2f033-c43d-93f8-a697-bf32eec3b550",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_variable_importance(train_X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "84f76826-572a-e65d-7dd7-cd3791bf5237"
   },
   "source": [
    "# 4. Modeling\n",
    "We will now select a model we would like to try then use the training dataset to train this model and thereby check the performance of the model using the test set. \n",
    "\n",
    "## 4.1 Model Selection\n",
    "Then there are several options to choose from when it comes to models. A good starting point is RandomForestClassifier. \n",
    "\n",
    "**Select the model you would like to try below and run the corresponding cell by pressing the play button.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier , GradientBoostingClassifier\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "bd56cc59-9abd-fb1f-e5e1-211c48776863"
   },
   "source": [
    "### 4.1.1 Random Forests Model\n",
    "Try a random forest model by running the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "29893a22-aa92-b35d-881e-7ce907f3b3b2"
   },
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "777e1893-585a-c545-250a-f6ea12c10fb9"
   },
   "source": [
    "### 4.1.2 Support Vector Machines\n",
    "Try a Support Vector Machines model by running the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c34e2a74-23b9-2916-683d-f0d6956ad5e6"
   },
   "outputs": [],
   "source": [
    "model = SVC()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "bc024820-b3db-4b2f-01aa-f9b35e413be1"
   },
   "source": [
    "### 4.1.3 Gradient Boosting Classifier\n",
    "Try a Gradient Boosting Classifier model by running the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "120f00c8-b568-f3af-97fd-df9762d4aefb"
   },
   "outputs": [],
   "source": [
    "model = GradientBoostingClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "452de3e7-c672-0aeb-b49e-5bb6c400de75"
   },
   "source": [
    "### 4.1.4 K-nearest neighbors\n",
    "Try a k-nearest neighbors model by running the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "862cf425-ab7c-6abb-3f86-fe6a63f4a790"
   },
   "outputs": [],
   "source": [
    "model = KNeighborsClassifier(n_neighbors = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "34eb3eed-6d89-5c9e-31e7-40eb7e01639e"
   },
   "source": [
    "### 4.1.5 Gaussian Naive Bayes\n",
    "Try a Gaussian Naive Bayes model by running the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e6b97cb8-56c1-3b01-ca52-c2e9685b68d0"
   },
   "outputs": [],
   "source": [
    "model = GaussianNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c3c89f76-4d2e-ae80-8b61-13feb6f33831"
   },
   "source": [
    "### 4.1.6 Logistic Regression\n",
    "Try a Logistic Regression model by running the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f3573fce-2045-aa1d-e010-dc28139b5a16"
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7e8d2a8d-a9f6-d416-74e7-4f5b711dcd98"
   },
   "source": [
    "## 4.2 Train the selected model\n",
    "When you have selected a dataset with the features you want and a model you would like to try it is now time to train the model. After all our preparation model training is simply done with the one line below.\n",
    "\n",
    "*Select the cell below and run it by pressing the play button.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "02a54d08-b02a-a4df-6d74-540be7d243d8"
   },
   "outputs": [],
   "source": [
    "model.fit( train_X , train_y )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3868397c-30ed-a870-9c61-30b5d5ebdce7"
   },
   "source": [
    "# 5. Evaluation\n",
    "Now we are going to evaluate model performance and the feature importance.\n",
    "\n",
    "## 5.1 Model performance\n",
    "\n",
    "We then compare this accuracy score with the accuracy when using the model on the training data. If the difference between these are significant this is an indication of overfitting. We try to avoid this because it means the model will not generalize well to new data and is expected to perform poorly.\n",
    "\n",
    "*Select the cell below and run it by pressing the play button.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c4e9a0a3-f637-5568-e92c-07c0548cd89d"
   },
   "outputs": [],
   "source": [
    "# Score the model for accuracy\n",
    "print (model.score( train_X , train_y ) , model.score( test_X , test_y ))\n",
    "# score model for f1\n",
    "print(f1_score(train_y, model.predict(train_X)), f1_score(test_y, model.predict(test_X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "cd75f2ca-dcb1-e2af-ebf8-e1dea0cf44a5"
   },
   "source": [
    "## 5.2 Feature importance - selecting the optimal features in the model\n",
    "We will now try to evaluate what the most important variables are for the model to make the prediction. The function below will only work for decision trees, so if that's the model you chose you can uncomment the code below (remove # in the beginning)  and see the feature importance.\n",
    "\n",
    "*Select the cell below and run it by pressing the play button.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f9412dd9-11ce-74ba-7fd6-148b2ae3bce3"
   },
   "outputs": [],
   "source": [
    "plot_model_var_imp(model, train_X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-Parameter tuning\n",
    "Most ML models have parameters that can be tuned to give better performance. For example logistic regression has regularisation coefient C, which can be adjusted to prevent overfitting. Lower C values -> more regularisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {'C': [0.01,0.05, 0.1, 0.5, 1, 5, 10, 100], 'penalty': ['l1', 'l2']}\n",
    "model_in = LogisticRegression()\n",
    "# reduced_x = train_X_full[['Family_Small', 'Family_Large', 'Pclass__2', 'Embarked__S', 'Family_Single', 'Embarked__C']]\n",
    "grid = GridSearchCV(model_in, param_grid=parameters, cv=2)\n",
    "grid.fit(train_X_full, train_y_full)\n",
    "best_model = grid.best_estimator_\n",
    "print(best_model.get_params())\n",
    "print(grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(**best_model.get_params())\n",
    "model.fit(train_X_full.astype('float'), train_y_full)\n",
    "print(model.score(train_X_full, train_y_full ), model.score( test_X , test_y ))\n",
    "print(f1_score(train_y_full, model.predict(train_X_full)), f1_score(test_y, model.predict(test_X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "61945225-1dcd-870d-bc49-851d5c91d1d5"
   },
   "source": [
    "## 5.3 Competition time!\n",
    "Who can build the model that gives the highest F1 score? Make imporvements in anyway you want.\n",
    "\n",
    "Test a bunch of different model and see which ones work best.\n",
    "\n",
    "Perform more hyper-parameter tuning on models.\n",
    "\n",
    "Maybe adding a ticket and cabin features will help you out .\n",
    "\n",
    "Reduce the number of features you use, this may help with overfitting. \n",
    "\n",
    "Many of the kaggle competition winners use an emsemble of models see (https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html) for more information on what I mean by that"
   ]
  }
 ],
 "metadata": {
  "_change_revision": 0,
  "_is_fork": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
